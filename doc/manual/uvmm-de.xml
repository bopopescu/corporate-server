<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE chapter [
	<!ENTITY % extensions SYSTEM "../stylesheets/macros.ent" >
	<!ENTITY % DocBookDTD PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
	"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
	<!ENTITY % entities SYSTEM "../stylesheets/macros-de.ent" >
	%extensions;
	%DocBookDTD;
	%entities;
]>
<chapter id="uvmm:chapter">
	<title>Virtualisierung</title>
	<section id="uvmm::introduction">
		<title>Einführung</title>
		<para>
			&ucsUVMM; (UVMM) ist ein Werkzeug für die Verwaltung hybrider Cloud-Umgebungen. Es können
			in der UCS-Domäne registrierte KVM-Virtualisierungsserver und darauf betriebene
			virtuelle Maschinen zentral überwacht und administriert werden. Zusätzlich können
			virtuelle Maschinen in OpenStack- oder EC2-Umgebungen administriert werden.
			Die Administration erfolgt über das &ucsUMC;-Modul <emphasis>Virtuelle Maschinen</emphasis>.
		</para>
		<para>
			In den virtualisierten Systemen kann im Prinzip jedes beliebige Betriebssystem verwendet werden.
		</para>
	</section>
	<section id="uvmm::installation">
		<title>Installation</title>

		<para>
			&ucsUVMM; kann mit der Applikation <emphasis>&ucsUVMM;</emphasis> aus dem Univention App Center installiert werden.
			Die Applikation kann auch direkt bei der Installation eines neuen UCS Servers ausgewählt werden.
			Alternativ kann das Softwarepaket <package>univention-virtual-machine-manager-daemon</package> installiert werden.
			Weitere Informationen finden sich in <xref linkend="computers::softwaremanagement::installsoftware"/>.
		</para>

		<para>
			Die Verwaltung von OpenStack Cloud-Instanzen ist direkt nach der Installation der Applikation mit dem &ucsUMC; Modul <emphasis>Virtuelle Maschinen (UVMM)</emphasis> möglich.
			Für die Verwaltung von virtuellen Maschinen in der Amazon EC2 Cloud muss die Applikation <emphasis>Amazon EC2 Cloud-Verbindung</emphasis> installiert werden.
		</para>
		<para>
			Um vor Ort einen KVM Virtualisierungsserver für die Verwaltung durch &ucsUVMM; hinzuzufügen, muss die Applikation <emphasis>KVM Virtualisierungsserver</emphasis> aus dem Univention App Center auf einem Server der Domäne installiert werden.
			Die Applikation kann auch direkt bei der Installation eines neuen UCS Servers ausgewählt werden.
			Alternativ kann das Softwarepaket <package>univention-virtual-machine-manager-node-kvm</package> installiert werden.
		</para>
		<para>
			Zum Betrieb von KVM wird zwingend CPU-Virtualisierungunterstützung benötigt.
			Diese wird von nahezu allen aktuellen x86 CPUs bereitgestellt.
			Zu Details kann die Webseite des KVM Projekts konsultiert werden:
			<ulink url="http://www.linux-kvm.org/"/>.
		</para>
		<para>
			Zusätzlich sollte bei der Installation eines Virtualisierungsservers die Architektur beachtet werden.
			Nur auf UCS-Systemen, die mit der amd64-Architektur installiert sind, können auch 64-Bit-Systeme virtualisiert werden.
			Für den Einsatz als Virtualisierungsserver wird die Verwendung eines 64-Bit-Systems (amd64) empfohlen.
		</para>

	</section>
	<section id="uvmm::cloudconnections">
		<title>Anlegen von Verbindungen zu Cloud Computing Instanzen</title>
		<para>
			&ucsUVMM; unterstützt Verbindungen zu OpenStack.
			Durch die Installation der Applikation <emphasis>Amazon EC2 Cloud-Verbindung</emphasis> ist auch das Verwalten von virtuellen Maschinen in der Amazon EC2 Cloud möglich.
		</para>
		<para>
			Um eine neue Verbindung anzulegen, muss das &ucsUMC; Modul <emphasis>Virtuelle Maschinen (UVMM)</emphasis> geöffnet werden.
			Durch einen Klick auf <guimenu>Erstellen</guimenu> öffnet sich ein Assistent, in dem der Punkt <guimenu>Erstellen einer neuen Cloud-Verbindung</guimenu> gewählt werden muss.
			Im nun verfügbaren Dropdown Feld kann die Art der Verbindung gewählt werden, mit einem Klick auf <guimenu>Weiter</guimenu> startet der Einrichtungsassistent.
			Sind die Einstellungen getätigt, wird mit einem Klick auf <guimenu>Fertigstellen</guimenu> die Verbindung hergestellt.
			Falls ein Fehler auftritt, wird dieser angezeigt und die Verbindungseinstellungen können korrigiert werden.
			Wenn die Verbindung erfolgreich hergestellt wurde, wird eine Warteanimation angezeigt, während alle verbindungsspezifischen Informationen der Cloud Verbindung geladen werden.
			Dies umfasst zum Beispiel die vorhandenen Instanzen und verfügbare Images, um neue Instanzen anlegen zu können.
		</para>
		<section id="uvmm:cloudconnections::openstack">
			<title>Anlegen einer OpenStack Verbindung</title>
			<figure id="uvmm-new-openstack-connection">
				<title>Anlegen einer Verbindung zu einer OpenStack Instanz</title>
				<graphic scalefit="1" width="90%" align="center" fileref="illustrations44/uvmm_new_openstack_connection_de.png"/>
			</figure>
			<para>
				Um eine Verbindung zu einer OpenStack Instanz herzustellen, sind im Einrichtungsassistenten folgende Einstellungen vorzunehmen:
				<table>
					<title>Felder bei der Einrichtung einer OpenStack Verbindung</title>
					<tgroup cols="2">
						<colspec colnum="1" colname="col1" colwidth="1*"/>
						<colspec colnum="2" colname="col2" colwidth="2*"/>
						<thead>
							<row>
								<entry>Attribut</entry>
								<entry>Beschreibung</entry>
							</row>
						</thead>
						<tbody>
							<row>
								<entry>Name</entry>
								<entry>
									Definiert den Namen der Verbindung.
									Dieser wird später in der Baumansicht des &ucsUMC; Moduls angezeigt.
								</entry>
							</row>
							<row>
								<entry>Benutzername</entry>
								<entry>
									Der Benutzername, der zur Authentifizierung an OpenStack benutzt werden soll.
								</entry>
							</row>
							<row>
								<entry>Folgenden Authentifizierungstyp nutzen</entry>
								<entry>
									Es kann zwischen zwei Werten gewählt werden. Der zugehörige Wert wird im darunterliegenden Feld eingegeben.
									<variablelist>
										<varlistentry>
											<term>Passwort</term>
											<listitem>
												<simpara>
													Das zum Benutzernamen gehörige Passwort.
												</simpara>
											</listitem>
										</varlistentry>
										<varlistentry>
											<term>API-Schlüssel</term>
											<listitem>
												<simpara>
													Der API-Schlüssel, der dem Benutzer Zugriff verschafft.
												</simpara>
											</listitem>
										</varlistentry>
									</variablelist>
								</entry>
							</row>
							<row>
								<entry>URL des Authentifizierungs-Endpunktes</entry>
								<entry>
									Hier ist die URL einzutragen, unter der der Authentifizierungs-Endpunkt der OpenStack Instanz erreichbar ist.
									Soll eine verschlüsselte Verbindung aufgebaut werden, ist die URL in der Form <uri>https://[...]</uri> anzugeben.
									Da für die verschlüsselte Verbindung das öffentliche Zertifikat der OpenStack Instanz verwendet wird, muss dieses dem UCS-System,
									auf dem die Applikation <emphasis>&ucsUVMM;</emphasis> installiert ist, verfügbar gemacht werden.
									Dazu muss das öffentliche Zertifikat in PEM Kodierung auf dem UCS-Server in das Verzeichnis <filename class="directory">/usr/local/share/ca-certificates/</filename> kopiert werden und mit der Endung <filename class="extension">.crt</filename> versehen sein.
									Die folgenden Kommandos konvertieren ein Zertifikat in die korrekte Kodierung und machen das Zertifikat bekannt:
									<programlisting language="sh">
openssl x509 -in [pfad/zum/openstack-zertifikat] \
	-outform pem -out /usr/local/share/ca-certificates/openstack.crt

update-ca-certificates
									</programlisting>
									Das öffentliche Zertifikat des OpenStack Authentifizierungs-Endpunktes ist der Konfiguration der OpenStack Instanz zu entnehmen.
									Der entsprechende Wert zum Pfad des Zertifikats ist in der <filename>keystone.conf</filename> unter <property>ca_certs</property> zu finden.
								</entry>
							</row>
							<row>
								<entry>Suchmuster für Images</entry>
								<entry>
									Wenn eine neue virtuelle Maschine erstellt werden soll, werden als Quell-Images nur solche Images angezeigt werden, die dem hier konfigurierten Suchmuster entsprechen.
									Durch den Standardwert "*" (Sternchen) werden alle verfügbaren Images angezeigt.
								</entry>
							</row>
							<row>
								<entry>Projekt / Tenant</entry>
								<entry>
									Der Projekt- oder Tenantname, der dem Benutzer innerhalb der OpenStack Umgebung zugewiesen ist.
								</entry>
							</row>
							<row>
								<entry>Serviceregion</entry>
								<entry>
									Der Name der Region, in der der Benutzer arbeiten soll. Der OpenStack Standardwert ist <userinput>regionOne</userinput>.
								</entry>
							</row>
							<row>
								<entry>Servicetyp</entry>
								<entry>
									Der Typ des Dienstes, unter dem die Cloud Compute Funktionalität zur Verfügung steht. Der Standardwert ist <userinput>compute</userinput>.
								</entry>
							</row>
							<row>
								<entry>Name des Dienstes</entry>
								<entry>
									Der Name des Dienstes, unter dem die Cloud Compute Funktionalität zur Verfügung steht. Der Standardwert ist <userinput>nova</userinput>.
								</entry>
							</row>
							<row>
								<entry>URL des Service-Endpunktes</entry>
								<entry>
									Optionaler Wert: Normalerweise wird die URL des Service-Endpunktes automatisch ermittelt, wenn sich der Benutzer an OpenStack anmeldet. Sollte die automatische Ermittlung nicht möglich sein, kann hier die entsprechende URL angegeben werden.
								</entry>
							</row>
						</tbody>
					</tgroup>
				</table>
			</para>
		</section>
		<section id="uvmm:cloudconnections::amazonec2">
			<title>Anlegen einer EC2 Verbindung</title>
			<para>
				Um eine Verbindung zu Amazon EC2 herzustellen, sind im Einrichtungsassistenten folgende Einstellungen vorzunehmen:
				<table>
					<title>Felder bei der Einrichtung einer Amazon EC2 Verbindung</title>
					<tgroup cols="2">
						<colspec colnum="1" colname="col1" colwidth="1*"/>
						<colspec colnum="2" colname="col2" colwidth="2*"/>
						<thead>
							<row>
								<entry>Attribut</entry>
								<entry>Beschreibung</entry>
							</row>
						</thead>
						<tbody>
							<row>
								<entry>Name</entry>
								<entry>
									Definiert den Namen der Verbindung.
									Dieser wird später in der Baumansicht des &ucsUMC; Moduls angezeigt.
								</entry>
							</row>
							<row>
								<entry>EC2 Region</entry>
								<entry>
									Hier wird ausgewählt, zu welcher EC2 Region die Verbindung aufgebaut werden soll.
									Virtuelle Maschinen sind immer genau einer Region zugeordnet und in anderen Regionen nicht sichtbar.
									Auch die Auswahl der verfügbaren Images kann sich zwischen den Regionen unterscheiden.
									Univention UCS Images sind in allen unterstützten Regionen verfügbar.
								</entry>
							</row>
							<row>
								<entry>Access Key ID</entry>
								<entry>
									Die Zugriffs-ID, die dem Amazon EC2 Konto zugeordnet ist, vergleichbar mit einem Benutzernamen.
								</entry>
							</row>
							<row>
								<entry>Geheimer Zugriffsschlüssel (Secret Access Key)</entry>
								<entry>
									Der geheime Schlüssel zum Zugriff über das Amazon EC2 Konto, vergleichbar mit einem Passwort.
								</entry>
							</row>
							<row>
								<entry>Suchmuster für AMIs</entry>
								<entry>
									Imagedateien als Quelle für neue Instanzen werden als AMI bezeichnet.
									Der hier angegebene Suchfilter beschränkt die Anzeige von auswählbaren AMIs beim anlegen einer neuen virtuellen Instanz.
									Durch den Wert "*" (Sternchen) werden alle verfügbaren Images angezeigt.
								</entry>
							</row>
						</tbody>
					</tgroup>
				</table>
			</para>
		</section>
	</section>
	<section id="uvmm::management">
		<title>Verwaltung virtueller Maschinen mit &ucsUMC;</title>
		<para>
			Die Verwaltung virtueller Maschinen im &ucsUVMM; erfolgt über das UMC-Modul <emphasis>Virtuelle Maschinen (UVMM)</emphasis>.
			Es bietet die Möglichkeit virtuelle Maschinen anzulegen, zu bearbeiten, zu löschen und den Status zu ändern.
			Diese Funktionen sind prinzipiell unabhängig von der eingesetzten Virtualisierungstechnik (vor Ort oder cloudbasiert), können sich aber im Detail unterscheiden.
			Was dabei zu beachten ist, wird in den folgenden Abschnitten zu den Beschreibungen der Funktionen erläutert.
		</para>
		<section id="uvmm::overview">
			<title>Operationen (Starten/Stoppen/Pausieren/Löschen/Migrieren/Klonen von virtuellen Maschinen)</title>
			<figure id="uvmm-overview">
				<title>Übersicht über die virtuellen Maschinen</title>
				<graphic scalefit="1" width="90%" align="center" fileref="illustrations44/uvmm_overview1_de.png"/>
			</figure>
			<para>
				Im Hauptdialog des UMC-Moduls wird auf der linken Seite eine Liste angezeigt, die einen Überblick über die vorhandenen Virtualisierungsserver und die konfigurierten Verbindungen zu Cloud Computing Systemen anzeigt.
				In der rechten Bildschirmhälfte werden alle virtuellen Maschinen aufgeführt.
				Klickt man auf den Namen eines Virtualisierungs-Servers oder einer Cloud Computing Verbindung, werden nur noch die zugehörigen virtuellen Maschinen dargestellt.
				Über die Suchmaske kann auch nach einzelnen virtuellen Maschinen gesucht werden.
			</para>
			<para>
				In der Übersichtsliste der virtuellen Maschinen kann anhand des Rechner-Icons erkannt werden, in welchem Status sich diese befindet, d.h. ob sie läuft (Rechnersymbol mit grünem Pfeil), gespeichert(Suspend) (Rechnersymbol mit gelben Längsstrichen) oder angehalten (Rechner ohne Zusatzsymbol) ist.
				Virtuelle Maschinen in Cloud Computing Umgebungen können zusätzlich als gelöscht (Rechner mit rotem Kreuz) oder als ausstehend (Rechner mit Sanduhr) dargestellt werden.
			</para>
			<para>
				Mit dem Icon, das einen Pfeil nach rechts darstellt, kann eine virtuelle Maschine gestartet werden.
			</para>

			<para>
				Auf laufende Maschinen auf KVM-Virtualisierungsservern kann - sofern konfiguriert - über das VNC-Protokoll zugegriffen werden.
				Das Icon mit der stilisierten Leinwand öffnet eine Verbindung über noVNC, einen HTML5-basierten VNC-Client.
				Für den Zugriff können auch beliebige andere VNC-Clients verwendet werden; der VNC-Zugriffsport wird in einem Tooltip über dem Rechnernamen angezeigt.
			</para>

			<para>
				Mit dem Auswahlfeld <guimenu>mehr</guimenu> können weitere Aktionen durchgeführt werden.
				Folgende Operationen stehen auf laufenden Maschinen zur Verfügung, die auf vor Ort laufenden KVM-Virtualisierungsservern betrieben werden:
			</para>
			<variablelist>
				<varlistentry>
					<term>Beenden</term>
					<listitem>
						<simpara>
						schaltet die Maschine aus.
						Dabei ist zu beachten, dass dabei das Betriebssystem der virtuellen Maschine vorher nicht runtergefahren wird, d.h. es mit dem Ausschalten eines Rechners zu vergleichen.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Pausieren</term>
					<listitem>
						<simpara>
						weist der Maschine keine weitere CPU-Zeit zu.
						Dadurch wird weiterhin der Arbeitsspeicher auf dem physikalischen Rechner belegt, die Maschine an sich aber angehalten.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Speichern und beenden</term>
					<listitem>
						<simpara>
						sichert den Inhalt des Arbeitsspeichers der Maschine auf Festplattenspeicher und weist der Maschine keine weitere CPU-Zeit zu, d.h. gegenüber <guimenu>Pausieren</guimenu> wird außerdem noch der Arbeitsspeicher freigegeben.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Migrieren</term>
					<listitem>
						<simpara>
						verschiebt die virtuelle Maschinen auf einen anderen Virtualisierungsserver.
						Weitere Hinweise finden sich in <xref linkend="uvmm::migration"/>.
						</simpara>
					</listitem>
				</varlistentry>
			</variablelist>
			<para>
				Folgende Operationen stehen auf gespeicherten oder ausgeschalteten Maschinen zur Verfügung:
			</para>
			<variablelist>
				<varlistentry>
					<term>Löschen</term>
					<listitem>
						<simpara>
						Nicht mehr benötigte virtuelle Maschinen können mitsamt ihrer Festplatten und ISO-Images gelöscht werden.
						Die zu löschenden Image-Dateien können dabei in einer Liste ausgewählt werden.
						Es ist zu beachten, dass ISO-Images und möglicherweise auch Festplatten-Images ggf. noch von anderen Maschinen verwendet werden.
						</simpara>

					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Migrieren</term>
					<listitem>
						<simpara>
						verschiebt die virtuelle Maschinen auf einen anderen Virtualisierungs-Server.
						Weitere Hinweise finden sich in <xref linkend="uvmm::migration"/>
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Klonen</term>
					<listitem>
						<simpara>
					  erzeugt eine Kopie der aktuellen VM. Die Kopie wird dabei mit einem frei
					  wählbaren, neuen Namen versehen. Die MAC-Adressen von Netzwerk-Interfaces
					  werden übernommen, können alternativ aber auch zufällig neu generiert
					  werden. Eingebundene CD- und DVD-Laufwerke der Quell-VM werden standardmäßig
					  in den Klon übernommen, während Festplatten kopiert werden, sofern der
					  Speicherbereich das Kopieren unterstützt. Sicherungspunkte werden nicht übernommen!
						</simpara>
					</listitem>
				</varlistentry>
			</variablelist>
			<para>
				Folgende Operationen stehen für virtuelle Maschinen zur Verfügung, die in cloudbasierten Umgebungen betrieben werden:
			</para>
			<variablelist>
				<varlistentry>
					<term>Neu starten (hard)</term>
					<listitem>
						<simpara>
							Startet die virtuelle Maschine neu, als wäre der Reset-Knopf betätigt worden.
							Hierbei kann es zu Datenverlust kommen.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Neu starten (soft)</term>
					<listitem>
						<simpara>
							Sendet ein ACPI-Reset Event an die virtuelle Maschine.
							Wenn das Betriebssystem der virtuellen Maschine dies korrekt interpretiert, wird ein geordneter Neustart durchgeführt.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Herunterfahren (soft)</term>
					<listitem>
						<simpara>
							Sendet ein ACPI-Shutdown Event an die virtuelle Maschine
							Wenn das Betriebssystem der virtuellen Maschine dies korrekt interpretiert, wird es geordnet heruntergefahren und ausgeschaltet.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Pausieren</term>
					<listitem>
						<simpara>
							Weist der Maschine keine weitere CPU-Zeit zu.
							Dadurch wird weiterhin der Arbeitsspeicher auf dem physikalischen Rechner belegt, die Maschine an sich aber angehalten.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Speichern und beenden</term>
					<listitem>
						<simpara>
							Sichert den Inhalt des Arbeitsspeichers der Maschine auf Festplattenspeicher und weist der Maschine keine weitere CPU-Zeit zu, d.h. gegenüber <guimenu>Pausieren</guimenu> wird außerdem noch der Arbeitsspeicher freigegeben.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Löschen</term>
					<listitem>
						<simpara>
							Schaltet die virtuelle Maschine aus und löscht alle dazugehörigen Daten unwiderruflich.
						</simpara>
					</listitem>
				</varlistentry>
			</variablelist>
		</section>
		<section id="uvmm::cloudinstanz::erstellen">
			<title>Erstellen einer virtuellen Maschine über eine Cloud Verbindung</title>
			<para>
				Virtuelle Maschinen in cloudbasierten Virtualisierungsumgebungen können in UVMM durch Klick auf <guimenu>Erstellen</guimenu> mit einem Assistenten in wenigen Schritten erstellt werden.
			</para>
			<para>
				In der Eingabemaske <guimenu>Erstellen einer virtuellen Maschine oder einer Cloud-Verbindung</guimenu> kann ausgewählt werden, auf über welche Cloud Verbindung die virtuelle Maschine angelegt werden soll.
				Nach der Auswahl einer Verbindung und einem Klick auf <guimenu>Weiter</guimenu>, gelangt man zum Assistenten zum anlegen einer neuen virtuellen Maschine.
				Nach dem Festlegen der Parameter wird die neue virtuelle Maschine nach einem Klick auf <guimenu>Fertigstellen</guimenu> angelegt.
			</para>
			<table>
				<title>Erstellen einer virtuellen Maschine über eine Cloud Verbindung</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="1*"/>
					<colspec colnum="2" colname="col2" colwidth="2*"/>
					<thead>
						<row>
							<entry>Attribut</entry>
							<entry>Beschreibung</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Name</entry>
							<entry>
								Definiert den Namen der virtuellen Maschine.
							</entry>
						</row>
						<row>
							<entry>Auswahl des Quell-Images / Quell-AMIs</entry>
							<entry>
								Der initiale Zustand einer virtuellen Maschine beim anlegen wird über ein Quell-Image (OpenStack) oder Quell-AMI (EC2) festgelegt.
								Ein solches Image enthält meist ein vorbereitetes Betriebssystem, dass vom Nutzer nach dem Start individualisiert werden kann.
								Es können beliebig viele virtuelle Maschinen aus einem Quell-Image erstellt werden.
							</entry>
						</row>
						<row>
							<entry>Wahl der Instanzgröße</entry>
							<entry>
								Einer virtuellen Maschine wird beim anlegen eine Instanzgröße zugeordnet. Diese setzt sich zusammen aus verfügbarem Arbeitsspeicher und der Größe des verfügbaren Festplattenspeichers.
								Beim Anlegen einer virtuellen Maschine in einer OpenStack Umgebung wird über die Größe auch die Anzahl der CPU-Kerne bestimmt.
							</entry>
						</row>
						<row>
							<entry>Auswahl eines Schlüsselpaares</entry>
							<entry>
								Um den sicheren Zugriff auf die virtuelle Maschine per ssh zu ermöglichen, wird der Maschine beim ersten Start ein ssh-Schlüssel zur Konfiguration des rootaccounts hinzugefügt.
								Mit diesem Schlüssel ist der ssh Zugriff auf die Maschine ohne Passwort möglich.
								Dazu muss der Zugriff auf den privaten Schlüsselteil des Schlüsselpaares bestehen.
								Der Zugriff auf die Instanz kann z.b. mit folgendem Aufruf erfolgen, wenn die Instanz läuft:
								<programlisting language="sh">
ssh -i [pfad/zum/privaten/schluessel] root@[ip-adresse-der-instanz]
								</programlisting>
							</entry>
						</row>
						<row>
							<entry>Konfigurieren einer Sicherheitsgruppe</entry>
							<entry>
								Diese Einstellung konfiguriert, welche Sicherheitsgruppe für die neue virtuelle Maschine gesetzt wird.
								Eine Sicherheitsgruppe bestimmt, welche TCP-Ports für den externen Zugriff auf eine virtuelle Maschine freigegeben werden.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
		<section id="uvmm::cloudinstanz::bearbeiten">
			<title>Bearbeiten einer virtuellen Maschine über eine Cloud Verbindung</title>
			<para>
				Durch Auswahl einer virtuellen Maschine und einem Klick auf <guimenu>Bearbeiten</guimenu> können auf einer separaten Seite die konfigurierten Einstellungen der virtuellen Maschine eingesehen werden.
				Insbesondere die IP-Adresse, über die die virtuelle Maschine erreichbar ist, ist hier einsehbar.
			</para>
		</section>
		<section id="uvmm::instanz::erstellen">
			<title>Erstellen einer virtuellen Maschine mit KVM</title>
			<para>
				Virtuelle Maschinen auf vor Ort betriebenen KVM Virtualisierungsservern können in UVMM durch einen Klick auf <guimenu>Erstellen</guimenu> mit einem Assistenten in wenigen Schritten erstellt werden.
			</para>
			<para>
				In der Eingabemaske <guimenu>Erstellen einer virtuellen Maschine oder einer Cloud-Verbindung</guimenu> kann ausgewählt werden, auf welchem Virtualisierungs-Server die virtuelle Maschine angelegt werden soll.
				Wird hier ein KVM Virtualisierungsserver ausgewählt und <guimenu>Weiter</guimenu> gewählt, gelangt man zur Auswahl des Maschinenprofils.
				Mit der Auswahl des <guimenu>Profil</guimenu>s werden einige grundlegende Einstellungen für die virtuelle Maschine vorgegeben werden (siehe <xref linkend="uvmm::profile"/>).
			</para>
			<para>
				Die virtuelle Maschine wird nun mit einem <guimenu>Namen</guimenu> und einer optionalen <guimenu>Beschreibung</guimenu> versehen.
				Anschließend wird der <guimenu>Arbeitsspeicher</guimenu> und die <guimenu>Anzahl der CPUs</guimenu> zugewiesen.
				Die Option <guimenu>Direktzugriff aktivieren</guimenu> legt fest, ob auf die Maschine über das VNC-Protokoll zugegriffen werden kann.
				Dies ist im Regelfall erforderlich für die initiale Betriebssysteminstallation.
			</para>
			<para>
				Nun werden die Laufwerke der virtuellen Maschine konfiguriert.
				Die Verwaltung von Laufwerken ist in <xref linkend="uvmm::imagefiles"/> dokumentiert.
			</para>
			<para>
				Ein Klick auf <guimenu>Fertigstellen</guimenu> schließt das Anlegen der virtuellen Maschine ab.
			</para>
		</section>
		<section id="uvmm-instance-edit">
			<title>Bearbeiten der Einstellungen einer virtuellen Maschine</title>
			<para>
				In der Übersichtsliste der virtuellen Maschinen kann durch Klick auf das Icon mit dem stilisierten Stift eine virtuelle Maschine bearbeitet werden.
			</para>
			<figure id="uvmm-drive">
				<title>Bearbeiten der Laufwerkseinstellung eines DVD-Laufwerks</title>
				<graphic scalefit="1" width="70%" align="center" fileref="illustrations44/uvmm_dvd_de.png"/>
			</figure>
			<para>
			  Die meisten Einstellungen einer virtuellen Maschine können nur verändert werden, wenn sie ausgeschaltet ist.
			</para>
			<table>
				<title>Reiter 'Allgemein'</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="1*"/>
					<colspec colnum="2" colname="col2" colwidth="2*"/>
					<thead>
						<row>
							<entry>Attribut</entry>
							<entry>Beschreibung</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Name</entry>
							<entry>
								Definiert den Namen der virtuellen Maschine.
								Dieser muss nicht mit dem Namen des Rechners im LDAP-Verzeichnis übereinstimmen.
							</entry>
						</row>
						<row>
							<entry>Betriebssystem</entry>
							<entry>
								Das in der virtuellen Maschine installierte Betriebssystem.
								Hier kann ein beliebiger Text eingetragen werden.
							</entry>
						</row>
						<row>
							<entry>Kontakt</entry>
							<entry>
								Definiert den Ansprechpartner für die virtuelle Maschine.
								Wird hier eine E-Mail-Adresse angegeben, so kann über die dann erscheinende Verknüpfung ein externes E-Mail-Programm aufgerufen werden.
							</entry>
						</row>
						<row>
							<entry>Beschreibung</entry>
							<entry>
								Hier kann eine beliebige Beschreibung hinterlegt werden, z.B. zur
								Funktion der virtuellen Maschine (<emphasis>Mailserver</emphasis>)
								oder zu deren Zustand. Die Beschreibung wird in der Übersicht der
								virtuellen Maschinen als Mouseover angezeigt.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<para>
			  Der Reiter <guimenu>Geräte</guimenu> erlaubt die Konfiguration der Laufwerke und Netzwerkschnittstellen.
			Eine Einführung zu den unterstützen Geräten, Speicherformaten und Speicherbereichen findet sich <xref linkend="uvmm::imagefiles"/>, zu den unterstützen Netzwerkkarten-Einstellungen in <xref linkend="uvmm:networkinterfaces"/>.
			</para>
			<para>
				Unter <guimenu>Laufwerke</guimenu> sind alle existierenden Laufwerke aufgeführt, die dabei verwendeten Image-Dateien, deren Größe und die zugeordneten Speicherbereiche.
				Mit dem Klick auf das stilisierte Minus-Zeichen kann das Laufwerk ausgehängt werden (die Image-Datei kann optional mitgelöscht werden).
			</para>
			<para>
				Mit <guimenu>Bearbeiten</guimenu> können Einstellungen nachträglich angepasst werden.
				Mit <guimenu>Paravirtualisiertes Laufwerk</guimenu> lässt sich festlegen, ob der Zugriff auf das Laufwerk paravirtualisiert erfolgen soll.
				Diese Einstellung sollte für eine virtuelle Maschine mit bereits installiertem Betriebssystem nach Möglichkeit nicht mehr verändert werden, da dann ggf. Partitionen nicht mehr angesprochen werden können.
			</para>
			<para>
				Werden zu einer existierenden Maschine weitere Laufwerke oder Netzwerkkarten hinzugefügt, wird die Verwendung von Paravirtualisierung anhand des referenzierten Profils oder aus den Eigenschaften der virtuellen Maschine über Heuristiken ermittelt.
			</para>
			<para>
				Mit <guimenu>Laufwerk hinzufügen</guimenu> kann ein weiteres Laufwerk hinzugefügt werden.
			</para>
			<para>
				Unter <guimenu>Netzwerkschnittstellen</guimenu> findet sich eine Liste aller Netzwerkkarten, die durch Anklicken der beiden Schaltflächen bearbeitet bzw. gelöscht werden können.
				Außerdem können über <guimenu>Hinzufügen einer Netzwerkschnittstelle</guimenu> neue Netzwerkkarten hinzugefügt werden.
			</para>

			<para>
				Im Reiter <guimenu>Sicherungspunkte</guimenu> findet sich eine Liste aller bestehenden Sicherungspunkte.
				Eine Einführung zu Sicherungspunkten findet sich in <xref linkend="uvmm::snapshots"/>.
				Mit <guimenu>Wiederherstellen</guimenu> kann auf einen alten Stand zurückgekehrt werden.
			</para>
			<caution>
				<para>
				Durch das Zurücksetzen auf einen alten Stand geht der aktuelle Stand verloren.
				Es spricht aber nichts dagegen, den aktuellen Stand zuvor in einem weiteren Sicherungspunkt zu sichern.
				</para>
			</caution>
			<para>
				Mit einem Klick auf das stilisierte Minus-Zeichen kann ein Sicherungspunkt entfernt werden.
				Der aktuelle Stand der Maschine bleibt davon unberührt.
			</para>
			<para>
				Mit <guimenu>Neuen Sicherungspunkt erstellen</guimenu> kann ein Sicherungspunkt unter einem frei wählbaren Namen erstellt werden, z.B. <emphasis>DC Master vor Update auf UCS 4.0-1</emphasis>.
				Zusätzlich wird der Zeitpunkt abgespeichert, zu dem der Sicherungspunkt erstellt wird.
			</para>

			<para>
				Im Reiter <guimenu>Zielrechner für Migration</guimenu> können die Hostsysteme konfiguriert werden, auf die die virtuelle Maschine migriert werden kann.
				Weitere Hinweise zur Migration finden sich in <xref linkend="uvmm::migration"/>.
			</para>

			<table>
				<title>Reiter 'Erweitert'</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="1*"/>
					<colspec colnum="2" colname="col2" colwidth="2*"/>
					<thead>
						<row>
							<entry>Attribut</entry>
							<entry>Beschreibung</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Architektur</entry>
							<entry>
								Legt die Architektur der emulierten Hardware fest.
								Dabei ist zu beachten, dass nur auf Virtualisierungsservern der Architektur amd64 virtuelle 64-Bit-Maschinen angelegt werden können.
								Diese Option wird auf i386-Systemen nicht angezeigt.
							</entry>
						</row>
						<row>
							<entry>Anzahl der CPUs</entry>
							<entry>
								Definiert wie viele CPU-Sockel der virtuellen Maschine zugeteilt werden.
								Die Anzahl der NUMA-Knoten, Cores und CPU-Threads ist derzeit nicht konfigurierbar.
							</entry>
						</row>
						<row>
							<entry>CPU Modell</entry>
							<entry>
								Das Modell der CPU für die virtuelle Maschine.
								Die Liste der nutzbaren Modelle hängt vom konkreten Hostsystem ab.
								Eine vollständige Liste der verfügbaren Modelle erhält man auf der Kommandozeile über <command>virsh domcapabilities</command>.
								Weitere Informationen - insbesondere zur Live-Migration - siehe <xref linkend="uvmm:migration:cpu"/>.
							</entry>
						</row>
						<row>
							<entry>Speicher</entry>
							<entry>
								Die Größe des zugewiesenen Arbeitsspeichers.
							</entry>
						</row>
						<row>
							<entry>RTC Referenz</entry>
							<entry>
								Bei vollvirtualisierten Systemen wird pro virtueller Maschine eine Rechneruhr emuliert (paravirtualisierte Systeme greifen direkt auf die Uhr des Virtualisierungsservers zurück).
								Diese Option speichert die Zeitzone der emulierten Uhr;
								sie kann entweder die <guimenu>Koordinierte Weltzeit (UTC)</guimenu> oder die <guimenu>lokalen Zeitzone</guimenu> verwenden.
								Für Linux-Systeme wird die Verwendung von UTC empfohlen, für Microsoft Windows-Systeme die Verwendung der lokalen Zeitzone.
							</entry>
						</row>
						<row>
							<entry>Bootreihenfolge</entry>
							<entry>
								Legt bei vollvirtualisierten Maschinen die Reihenfolge fest, in der das emulierte BIOS der virtuellen Maschine die Laufwerke nach bootbaren Medien durchsucht.

								Bei paravirtualisierten Maschinen kann lediglich eine Festplatte ausgewählt werden, aus der der Kernel benutzt werden soll.
							</entry>
						</row>
						<row>
							<entry>Hyper-V Enlightenment</entry>
							<entry>
								Erlaubt es Gastsystemen wie <systemitem class="osname">Microsoft Windows</systemitem> effizienter als virtuelle Maschine zu laufen.
							</entry>
						</row>
						<row>
							<entry>VM immer mit Host starten</entry>
							<entry>
								Definiert, ob die virtuelle Maschine jedesmal automatisch gestartet werden soll, wenn das Hostsystem selber startet.
							</entry>
						</row>
						<row>
							<entry>Direktzugriff (VNC)</entry>
							<entry>
							  Definiert, ob der VNC-Zugriff zur virtuellen Maschine aktiviert werden
							  soll. Ist die Option aktiv, kann über das UMC-Modul durch einen
							  HTML5-basierten VNC-Client - oder einen beliebigen anderen Client -
							  direkt auf die virtuelle Maschine zugegriffen werden. Die VNC-URL wird in einem Tooltip angezeigt.
							</entry>
						</row>
						<row>
							<entry>Global verfügbar</entry>
							<entry>
								Erlaubt den VNC-Direktzugriff auch von anderen Systemen als dem Virtualisierungsserver.
							</entry>
						</row>
						<row>
							<entry>VNC Passwort</entry>
							<entry>
								Setzt ein Passwort für die VNC-Verbindung.
							</entry>
						</row>
						<row>
							<entry>Tastaturlayout</entry>
							<entry>
								Legt das Layout für die Tastatur in der VNC-Sitzung fest.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>

	<section id="uvmm::kvmfeatures">
		<title>KVM-bezogene Merkmale von UVMM</title>
		<section id="uvmm::imagefiles">
			<title>Image-Dateien virtueller Maschinen</title>
				<!--
					Bitte zuerst Kapitel 'Verwaltung virtueller Maschinen mit der Univention Management Console'
				-->
			<para>
				Werden virtuelle Festplatten zu einer Maschine hinzugefügt, werden im Regelfall für die Datenhaltung <emphasis>Image-Dateien</emphasis> verwendet.
				Eine Image-Datei kann entweder neu erzeugt werden oder eine bereits vorhandene Image-Datei einer virtuellen Maschine zugewiesen werden.
				Alternativ kann einer virtuellen Maschine auch ein natives Block-Device (Festplattenpartition, Logical-Volume, iSCSI-Volume) zugewiesen werden.
				Die direkte Verwendung von Block-Devices bietet Performance-Vorteile und ist weniger anfällig gegen Rechnerabstürze.
			</para>
			<para>
				Auf KVM-Systemen können Image-Dateien in zwei Formaten verwaltet werden:
				Standardmäßig werden sie im <guimenuitem>Erweiterten Format (qcow2)</guimenuitem> angelegt.
				Dieses unterstützt Copy-on-write, was bedeutet, dass eine Änderung nicht das Original überschreibt, sondern die neue Version stattdessen an einer anderen Position abgelegt wird.
				Die interne Referenzierung wird dann so aktualisiert, dass wahlweise sowohl die Originalversion als auch die neue Version zugreifbar sind.
				Nur bei Verwendung von Festplatten-Images im <guimenuitem>Erweiterten Format</guimenuitem> können Sicherungspunkte erstellt werden.
				Alternativ kann auch im <guimenuitem>Einfachen Format (raw)</guimenuitem> auf ein Festplatten-Image zugegriffen werden.
			</para>

			<para>
			  Zur Beschleunigung von Zugriffen auf Speichermedien verwenden Betriebssysteme einen
			  sogenannten <emphasis>Page Cache</emphasis>. Wenn auf Daten zugegriffen wird, die vorher
			  schon von einer Festplatte gelesen wurden und diese im Cache noch vorhanden sind, entfällt
			  ein vergleichsweise langsamer Zugriff auf das Speichermedium und die Anfrage wird aus dem
			  Page Cache bedient.
			</para>

			<para>
			  Schreibzugriffe werden in der Regel auch nicht unmittelbar auf die Festplatte geschrieben,
			  sondern oft gebündelt und dadurch effizienter geschrieben. Dies birgt allerdings die
			  Gefahr eines Datenverlustes, wenn z.B. ein System abstürzt oder die Stromversorgung
			  unterbrochen wird: Die Daten, die bis dahin nur im Schreibcache vorgehalten wurden und
			  noch nicht auf das Speichermedium synchronisiert wurden, sind dann verloren. Bei modernen
			  Betriebssystemen wird in der Regel dafür gesorgt, dass anstehende Schreibänderungen nach
			  maximal einigen Sekunden auf die Festplatte geschrieben werden.
			</para>

			<para>
			  Um zu vermeiden, das Daten sowohl im Page Cache des Wirtsystems als auch des Gastssystems
			  doppelt vorgehalten werden, können mit der Option
			  <guimenu>Caching</guimenu> verschiedene Cache-Strategien konfiguriert werden, die die
			  Verwendung des Page Caches des Wirtsystems beeinflussen:
			</para>

			<itemizedlist>
				<listitem>
					<simpara>
				  Die Grundeinstellung seit UCS-3.1 ist <guimenuitem>none</guimenuitem>: Dabei greift KVM
				  direkt auf die Festplatte zu und umgeht den Page Cache auf dem
				  Virtualisierungsserver.<!--footnote><para>Diese Strategie funktioniert nicht mit Dateisystemen von Typ <literal>tmpfs</literal> und bei einigen <acronym>NFS</acronym>-Servern.</para></footnote--> Lesezugriffe werden jedesmal direkt von der Festplatte
				  beantwortet und Schreibzugriffe direkt an die Festplatte durchgereicht.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  Mit der Strategie <guimenuitem>write-through</guimenuitem> wird der Page Cache auf dem Virtualisierungsserver
				  benutzt, jedoch wird jeder Schreibzugriff auch direkt an das Speichermedium
				  durchgereicht. Auf Virtualisierungsservern mit viel freiem Hauptspeicher können
				  Lesezugriffe gegenüber <guimenuitem>none</guimenuitem> effizienter sein. I.d.R. wirkt sich das doppelte Caching
				  aber eher negativ auf die Gesamtperformance aus

				  <footnote><para>Es empfiehlt sich eher, den freien Speicher den VMs direkt zur Verfügung zu stellen, so dass diese diesen zusätzlichen Speicher selbst effizienter nutzen können, u.a. auch zum Cachen.</para></footnote>.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  Wird die Strategie <guimenuitem>write-back</guimenuitem> verwendet, wird der Page Cache des
				  Hosts sowohl für Lese- als auch für Schreibzugriffe genutzt. Schreibzugriffe werden
				  zunächst nur im Page Cache durchgeführt, bevor dieser dann irgendwann später erst auf
				  die Festplatte geschrieben wird. Ein Crash des Hostsystems kann dadurch zu Datenverlusten
				  führen.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  Mit der Strategie <guimenuitem>unsafe</guimenuitem> werden Synchronisationsanforderungen
				  ignoriert, die vom Gastsystem gesendet werden, um explizit das Schreiben ausstehender
				  Daten auf das Speichermedium zu erzwingen. Dies erhöht gegenüber
				  <guimenuitem>write-back</guimenuitem> abermals die Performance, führt aber bei einem Crash des
				  Hostsystems zu Datenverlust. Diese Variante ist nur für Testsysteme oder vergleichbare
				  Installationen sinnvoll, in denen ein Datenverlust durch einen Absturz des Hostsystem
				  verschmerzbar ist.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  Die Strategie <guimenuitem>directsync</guimenuitem> entspricht <guimenuitem>none</guimenuitem>,
				  nur dass hier nach jedem Schreibzugriff nochmals explizit eine Synchronisation
				  erzwungen wird.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  Die Option <guimenuitem>Hypervisor-Standard</guimenuitem> ist abhängig von der UCS-Version
				  und der KVM-Version, mit der ein Gastsystem installiert wurde: Ursprünglich war der
				  Standardwert bis UCS 3.0 implizit <guimenuitem>write-through</guimenuitem>, aber mit UCS 3.1 wurde KVM so
				  modifiziert, dass für alte VMs jetzt statt dessen <guimenuitem>none</guimenuitem> verwendet wird. Bei mit
				  UCS 3.1 neu angelegten VMs entspricht der Standardwert wieder implizit
				  <guimenuitem>write-through</guimenuitem>, allerdings werden neue VMs explizit mit <guimenuitem>none</guimenuitem> angelegt.
					</simpara>
				</listitem>
			</itemizedlist>

			<para>
				Wenn eine Live-Migration virtueller Maschinen zwischen verschiedenen Virtualisierungsservern erfolgen soll, muss der Speicherbereich auf einem System abgelegt werden, auf das alle Virtualisierungsserver zugreifen können (z.B. eine NFS-Freigabe oder ein iSCSI-Target).
				Dies wird in <xref linkend="uvmm::defaultpool"/> beschrieben.
			</para>
			<para>
				Festplatten-Images werden mit der angegebenen Größe als Sparse-Datei angelegt, d.h. diese Dateien wachsen erst bei der Verwendung bis zur maximal angegebenen Größe und benötigen initial nur geringen Speicherplatz.
				Da hierbei die Gefahr besteht, dass dadurch im laufenden Betrieb der Speicherplatz erschöpft ist, sollte eine Nagios-Überwachung integriert werden, siehe <xref linkend="nagios::general"/>.
			</para>
			<para>
				Festplatten-Images sollten nach Möglichkeit paravirtualisiert angesprochen werden.
				Bei UCS-Systemen, die virtualisiert unter KVM installiert werden, wird durch die Auswahl des UCS-Profils automatisch ein paravirtualisierter Zugriff aktiviert.
				Die Konfiguration von Microsoft Windows-Systemen ist in <xref linkend="uvmm::gplpvvirtio"/> dokumentiert.
			</para>
		</section>

		<section id="uvmm::storagepools">
		  <title>Speicherbereiche</title>

		  <para>
			Image-Dateien werden in sogenannten Speicherbereichen abgelegt. Diese können entweder
			lokal auf dem Virtualisierungsserver oder auf einer Freigabe abgelegt werden. Die Anbindung
			eines Speicherbereichs über iSCSI ist in <xref linkend="ext-doc-uvmm"/> beschrieben.
		  </para>

		  <section id="uvmm::defaultpool">
			<title>Zugriff auf den Standard-Speicherbereich über eine Freigabe</title>
			<para>
			  Jeder Virtualisierungsserver stellt in der Voreinstellung einen Speicherbereich mit dem Namen
			  <wordasword>default</wordasword> zur Verfügung. Dieser liegt auf den Virtualisierungsservern
			  unterhalb des Verzeichnisses <filename class="directory">/var/lib/libvirt/images/</filename>.
			</para>

			<para>
			  Um einen einfachen Zugriff auf den Speicherbereich zu ermöglichen, kann eine Freigabe für
			  das Verzeichnis <filename class="directory">/var/lib/libvirt/images/</filename> eingerichtet werden. Dazu
			  muss im UMC-Modul <guimenu>Freigaben</guimenu> eine Freigabe mit den folgenden Optionen
			  angelegt werden. Auf die Freigabe kann dann anschließend einfach von Windows-Clients über eine
			  CIFS-Netzwerkfreigabe (oder auch über einen NFS-Mount) zugegriffen werden.
			</para>
			<itemizedlist>
				<listitem>
					<para>Allgemein/Grundeinstellungen</para>
					<itemizedlist>
						<listitem><simpara>Name: <userinput>UVMM-Pool</userinput></simpara></listitem>
						<listitem><simpara>Server: Der Rechnername des UVMM-Servers</simpara></listitem>
						<listitem><simpara>Pfad: <userinput>/var/lib/libvirt/images</userinput></simpara></listitem>
						<listitem><simpara>Verzeichnis-Besitzer, Verzeichnis-Gruppe und Verzeichnismodus können beibehalten werden</simpara></listitem>
					</itemizedlist>
				</listitem>
				<listitem>
					<para>Erweiterte Einstellungen/Samba-Rechte</para>
					<itemizedlist>
						<listitem><simpara>Gültige Benutzer oder Gruppen: <userinput>Administrator</userinput></simpara></listitem>
					</itemizedlist>
				</listitem>
			</itemizedlist>
			<para>
			  Die Image-Dateien einer virtuellen Festplatten enthalten sämtliche Nutzdaten des
			  virtualisierten Systems! Die Option <guimenu>Gültige Benutzer oder Gruppen</guimenu> stellt sicher,
			  dass unabhängig von den Dateisystemberechtigungen nur der Administrator-Benutzer auf die
			  Freigabe zugreifen kann.
			</para>
		  </section>

		  <section id="uvmm::addpool">
			<title>Hinzufügen eines Speicherbereichs</title>
			<para>
			  Ein weiterer Speicherbereich kann nicht über &ucsUMC; angelegt werden. Stattdessen muss eine
			  Anmeldung als Benutzer <systemitem class="username">root</systemitem> auf einen Virtualisierungs-Server
			  erfolgen. Die folgenden Schritte sind dafür nötig:

			  <itemizedlist>
				<listitem><simpara>
				  Das Verzeichnis, in dem die Daten des Speicherbereichs abgelegt werden sollen, muss
				  angelegt werden, in diesem Beispiel <filename class="directory">/mnt/storage/</filename>.
				</simpara></listitem>

				<listitem><simpara>
				  Mit dem folgenden Befehl wird der neue Speicherbereich <wordasword>Testpool</wordasword> erstellt:
				  </simpara>
				  <programlisting language="sh">
virsh pool-define-as Testpool dir - - - - "/mnt/storage"
				  </programlisting>
				</listitem>

				<listitem><simpara>
				  Die von UVMM verwendete Bibliothek libvirt unterscheidet zwischen aktiven und
				  inaktiven Speicherbereichen. Um den Speicherbereich direkt verwenden zu können, muss er
				  aktiviert werden:
				  </simpara>
				  <programlisting language="sh">
virsh pool-start Testpool
				  </programlisting>
				  <simpara>
				  Der folgende Befehl stellt sicher, dass der Pool automatisch beim nächsten Systemstart
				  aktiviert wird:
				  </simpara>
				  <programlisting language="sh">
virsh pool-autostart Testpool
				  </programlisting>
				</listitem>
			  </itemizedlist>
			</para>
		  </section>

		  <section id="uvmm::movepool">
			<title>Verschieben des default-Speicherbereichs</title>
			<para>
			  Um den unterliegenden Dateipfad eines Speicherbereichs nachträglich zu ändern, muss eine
			  Anmeldung als Benutzer <systemitem class="username">root</systemitem> auf dem Virtualisierungs-Server
			  erfolgen. Die folgenden Schritte sind dafür nötig:

			  <itemizedlist>
				<listitem><simpara>
				  Die &ucsUCRV; <envar>uvmm/pool/default/path</envar> muss auf das neue Verzeichnis geändert werden.
				</simpara></listitem>

				<listitem><simpara>
				  Die folgenden Befehle entfernen den alten Speicherbereich; durch einen Neustart des
				  UVMM wird der Speicherbereich unter dem neuen Pfad angelegt:
				  </simpara>
				  <programlisting language="sh">
virsh pool-destroy default
virsh pool-undefine default
invoke-rc.d univention-virtual-machine-manager-daemon restart
invoke-rc.d univention-virtual-machine-manager-node-common restart
				  </programlisting>
				</listitem>
			  </itemizedlist>
			</para>
		  </section>
		</section>

		<section id="uvmm::drives">
			<title>CD/DVD/Disketten-Laufwerke in virtuellen Maschinen</title>

			<para>
				CD-/DVD-ROM-/Disketten-Laufwerke können auf zwei Arten eingebunden werden:
			</para>
			<itemizedlist>
				<listitem>
					<simpara>
					Aus einem Speicherbereich kann ein ISO-Image zugewiesen werden. Wurde kein
					zusätzlicher Speicherbereich angelegt, werden die ISO-Dateien im Speicherbereich
					<emphasis>default</emphasis> aus dem Verzeichnis
					<filename class="directory">/var/lib/libvirt/images/</filename> ausgelesen.
					</simpara>
				</listitem>
				<listitem>
					<simpara>
					Alternativ kann auch ein physisches Laufwerk des Virtualisierungsservers mit der virtuellen Maschine verbunden werden.
					</simpara>
				</listitem>
			</itemizedlist>
			<para>
				Ein Diskettenlaufwerk kann einer virtuellen Maschine ebenfalls über ein Image (im <filename class="extension">VFD</filename>-Format) oder durch Durchreichung eines physischen Laufwerks bereitgestellt werden.
			</para>
			<para>
				Werden Laufwerke für eine neu zu installierende Maschine definiert, muss sichergestellt werden, dass von dem CD-ROM-Laufwerk gebootet wird.
				Das UVMM-Profil (siehe <xref linkend="uvmm::profile"/>) gibt die Bootreihenfolge für vollvirtualisierte Maschinen bereits vor.
				Bei paravirtualisierten Maschinen wird es durch die Reihenfolge bei der Definition der Laufwerke festgelegt und kann auch nachträglich in den Einstellungen angepasst werden.
			</para>
		</section>


		<section id="uvmm:networkinterfaces">
			<title>Netzwerk-Karten virtueller Maschinen</title>
			<para>
				Beim Anlegen einer virtuellen Maschine wird dieser automatisch eine Netzwerkkarte mit zufällig erstellter MAC-Adresse zugewiesen.
				Diese kann ggf. nachträglich verändert werden.
			</para>
			<para>
				Zwei Typen von Netzwerkverbindung sind möglich:
			</para>
			<itemizedlist>
				<listitem>
					<simpara>
					In der Grundeinstellung wird mit einer <emphasis>Bridge</emphasis> auf dem Virtualisierungsserver direkt auf das Netz zugegriffen.
					Die virtuelle Maschine verwendet dabei ihre eigene IP-Adresse und ist damit auch von anderen Rechnern aus erreichbar.
					</simpara>
				</listitem>
				<listitem>
					<simpara>
					Netzwerkkarten vom Typ <emphasis>Network Address Translation (NAT)</emphasis> werden in einem privaten Netz auf dem Virtualisierungsserver definiert.
					Dabei muss der virtuellen Maschine eine IP-Adresse aus dem Netz <systemitem class="ipaddress">192.0.2.0/24</systemitem> gegeben werden.
					Über NAT wird dieser virtuellen Maschine der Zugang zum externen Netz erteilt, so dass der Zugriff über die IP-Adresse des Virtualisierungsservers erfolgt.
					Die virtuelle Maschine ist damit nicht von anderen Rechnern erreichbar, kann aber selber beliebige ausgehende Verbindungen aufbauen.
					</simpara>
				</listitem>
			</itemizedlist>
			<figure id="uvmm-network">
				<title>Hinzufügen einer virtuellen Netzwerkkarte</title>
				<graphic scalefit="1" width="70%" align="center" fileref="illustrations44/uvmm_network_de.png"/>
			</figure>
			<para>
				Die UVMM-Server sind für NAT und Bridging vorkonfiguriert.
				Allerdings gibt es Einschränkungen für Netzwerkkarten vom Typ Bridge, welche in <xref linkend="computers:networkcomplex:uvmm"/> beschrieben sind.
				Für jede virtuelle Maschine kann das zu verwendende Netzwerk über die Option <guimenu>Quelle</guimenu> ausgewählt werden.
			</para>
			<para>
				Netzwerkkarten vom Typ NAT sind nur durch die im Netz <systemitem class="ipaddress">192.0.2.0/24</systemitem> verfügbaren IP-Adressen begrenzt.
			</para>
			<para>
				Über die Option <guimenu>Treiber</guimenu> kann ausgewählt werden, welche Art von Netzwerkkarte bereitgestellt wird.
				Die <guimenuitem>Realtek RTL-8139</guimenuitem> wird von nahezu jedem Betriebssystem unterstützt, die <guimenuitem>Intel Pro-1000</guimenuitem> bietet erweiterte Fähigkeiten und ein <guimenuitem>Paravirtualisiertes Gerät</guimenuitem> die beste Performance.
			</para>
		</section>
		<section id="uvmm::gplpvvirtio">
			<title>Paravirtualisierung (virtIO)-Treiber für Microsoft Windows-Systeme</title>
			<para>
				KVM unterstützt Paravirtualisierung über die virtIO Schnittstelle.
				Durch die Verwendung von Paravirtualisierung können die virtualisierten Systeme einen direkten Zugriff auf die Ressourcen des Virtualisierungsservers erhalten.
				Dies verbessert die Performance erheblich.
				Die Verwendung von Paravirtualisierung wird empfohlen.
			</para>
			<para>
				Aktuelle Linux-Systeme unterstützen standardmäßig Paravirtualisierung.
				Mit der Installation der KVM-Pakete werden passende Images bereitgestellt, die dann in der Laufwerksverwaltung in eine virtuelle Maschine eingebunden werden können.
				Die Images werden in den mit der &ucsUCRV; <envar>uvmm/pool/default/path</envar> festgelegten Speicherbereich integriert:
				Auf KVM-Virtualisierungsservern wird ein ISO-Image mit dem Namen <emphasis>KVM Windows drivers</emphasis> bereitgestellt, dass die virtIO-Virtualisierungstreiber für Microsoft Windows enthält.
			</para>
			<section id="uvmm:virtio">
			  <title>Installation der virtIO-Treiber für KVM-Instanzen</title>
			  <para>
				Bei Windows-Systemen, die unter KVM installiert werden muss <emphasis>vor</emphasis>
				Beginn der Windows-Installation Paravirtualisierung für die verwendeten Festplatten aktiviert werden.
			  </para>
			  <para>
				Die virtIO-Schnittstelle erlaubt einer virtuellen Maschine den effizienten Zugriff auf
				Netzwerk- und Speicher-Ressourcen des KVM-Hypervisors. Die folgenden Schritte
				beschreiben die Einrichtung der virtIO-Treiber unter Windows 7.
			  </para>

			  <itemizedlist>
				<listitem><simpara>
				  Ein CDROM/DVD-Laufwerk muss eingerichtet und das Image <guimenuitem>KVM Windows drivers</guimenuitem> zugewiesen werden.
				</simpara></listitem>

				<listitem><simpara>
				  Für die Festplatten-Laufwerke muss im <guimenu>Geräte</guimenu>-Dialog des &ucsUVMM;
				  die Option <guimenuitem>Paravirtualisiertes Laufwerk</guimenuitem> aktiviert werden.
				</simpara></listitem>

				<listitem><simpara>
				  Für die Netzwerkkarte(n) muss der <guimenu>Treiber</guimenu> auf
				  <guimenuitem>Paravirtualisiert (virtio)</guimenuitem> konfiguriert werden.
				</simpara></listitem>

				<listitem><simpara>
				  Die initialen Schritte der Windows-Installation sind unverändert. Im
				  Partitionierungs-Dialog erscheint die Warnung, dass nicht auf Massenspeicher
				  zugegriffen werden kann; dies stellt keinen Fehler dar. Im selben Menü können die
				  virtIO-Treiber mit <guimenu>Treiber laden</guimenu> eingerichtet werden. Unter Windows
				  7 (und ebenso Windows 2003/2008) muss <guimenuitem>Red Hat virtIO SCSI
				  Controller</guimenuitem> und <guimenuitem>Red Hat virtIO Ethernet Adapter</guimenuitem>
				  ausgewählt werden. Nach der Treiber-Installation ist die Festplatte im
				  Windows-Installationsdialog sichtbar und die Installation kann fortgesetzt werden.
				</simpara></listitem>

				<listitem><simpara>
				  Nach Abschluss der Installation werden die Geräte <computeroutput>Red Hat virtIO SCSI Disk
				  Device</computeroutput> und <computeroutput>Red Hat virtIO Ethernet Adapter</computeroutput> im
				  Windows-Gerätemanager angezeigt.
				</simpara></listitem>
			  </itemizedlist>
			</section>
		  </section>
		<section id="uvmm::snapshots">
			<title>Sicherungspunkte</title>
			<para>
				UVMM bietet die Möglichkeit, den Inhalt von Arbeits- und Festplattenspeicher einer virtuellen Maschine in Sicherungspunkten zu speichern.
				Zu diesen kann später wieder zurückgewechselt werden, was gerade bei Software-Updates ein nützliches "Sicherungsnetz" darstellt.
			</para>
			<para>
				Sicherungspunkte können nur mit Instanzen verwendet werden, deren Festplatten-Images ausschließlich das qcow2-Format verwenden.
				Alle Sicherungspunkte werden dabei im Copy-on-write-Verfahren (siehe <xref linkend="uvmm::instanz::erstellen"/>) direkt in den Festplatten-Image-Dateien gespeichert.
			</para>
		</section>
		<section id="uvmm::migration">
			<title>Migration virtueller Maschinen</title>
			<para>
				UVMM bietet die Möglichkeit eine virtuelle Maschine von einem auf einen anderen physikalischen Server zu migrieren.
				Dies funktioniert sowohl mit ausgeschalteten, wie auch mit laufenden Maschinen (Live-Migration).
				Die Option wird nur angeboten, wenn sich min. zwei Virtualisierungsserver in der Domäne befinden.
				Die Migration von Instanzen zwischen Cloud Computing Umgebungen oder vor Ort Virtualisierungsservern ist nicht möglich.
			</para>
			<figure id="uvmm-migrate">
				<title>Migrieren einer virtuellen Maschine</title>
				<graphic scalefit="1" width="40%" align="center" fileref="illustrations44/uvmm_migrate_de.png"/>
			</figure>

			<para>
			  Bei der Migration ist zu beachten, dass die Image-Dateien der eingebundenen Festplatten
			  und CD-ROM-Laufwerk von beiden Virtualisierungsservern zugreifbar sein müssen. Dies kann
			  beispielsweise dadurch realisiert werden, dass die Images auf einem zentralen Storage
			  abgelegt werden. Hinweise zur Einrichtung einer solchen Umgebung finden sich unter <xref
			  linkend="uvmm::storagepools"/>.
			</para>

			<section id="uvmm:Migration_virtueller_Maschinen_ausgefallener_Virtualisierungsserver">
				<title>Migration virtueller Maschinen ausgefallener Virtualisierungsserver</title>
				<para>
					Die Konfigurationen der virtuellen Maschinen aller Virtualisierungsserver werden zentral durch &ucsUVMM; erfasst.
					Ist ein Server ausgefallen (die Ausfallerkennung erfolgt periodisch alle fünfzehn Sekunden), wird der Server und die darauf betriebenen virtuellen Maschinen mit einem roten Symbol als unerreichbar markiert, eine Warnmeldung angezeigt und als einzige Operation das <guimenu>Migrieren</guimenu> der virtuellen Maschine angeboten.
				</para>
				<para>
					Nach der Migration wird die virtuelle Maschine in UVMM auf dem ausgefallenen Virtualisierungsservers lediglich ausgeblendet und bleibt dort weiterhin definiert.
				</para>
				<caution>
					<simpara>
					Es ist unbedingt sicherzustellen, dass die virtuelle Maschine auf dem Ursprungs- und dem Ausweichserver nicht parallel gestartet sind, da ansonsten beide gleichzeitig in die selben Image-Dateien schreiben, was zu Datenverlusten führt.
					Falls virtuelle Maschinen nach dem Start automatisch gestartet werden, sollte durch Trennen der Netzwerkverbindung oder Einschränkung des Zugriffs auf den Speicherbereich ein gemeinsamer Zugriff unbedingt verhindert werden.
					</simpara>
				</caution>
				<para>
					Falls der ausgefallene Rechner wieder aktiviert wird, - z.B. weil die Stromversorgung nur temporär unterbrochen war - sind die virtuellen Maschinen weiterhin lokal auf dem System definiert und werden erneut an UVMM gemeldet, d.h. die Maschine wird dann doppelt angezeigt.
				</para>
				<para>
					Deshalb sollte anschließend eine der beiden Maschinen entfernt werden.
					Die verwendeten Image-Dateien der Laufwerke sollten dabei <emphasis>nicht</emphasis> mitgelöscht werden.
				</para>
			</section>
			<section id="uvmm:migration:cpu">
				<title>Migration von virtuellen Maschinen zwischen Servern mit unterschiedlichen CPUs</title>
				<para>
					Virtuelle Maschinen können zwischen Servern mit kompatiblen CPUs migriert werden.
					Neuere CPUs sind normalerweise abwärtskompatibel mit vorherigen Generationen der CPU und fügen nur neuere Funktionen hinzu.
					Die Umkehrung dagegen ist nicht wahr:
					Falls das Gast-Betriebssystem sich entschieden hat, eine neuere Funktion zu nutzen, und diese nach der Migration nicht mehr zur Verfügung steht, wird die virtuelle Maschine abstürzen.
				</para>
				<para>
					Standardmäßig ist kein bestimmtes CPU-Modell explizit konfiguriert:
					Der virtuellen Machine werden vielmehr direkt die CPU-Funktionen des jeweiligen Virtualisierungsserver durchgereicht.
					Der Vorteil ist, dass die Performance höher ist, der Nachteil ist, dass es dadurch zu Abstürzen bei der Live-Migration kommen kann.
					Um die Migration zwischen inkompatiblen CPUs zu verhindern kann UVMM das CPU-Modell der Server beachten.
					Diese Funktionalität muss pro virtueller Maschine konfiguriert werden und wird erst nach einem Neustart der virtuellen Machine wirksam.
					Ein Reboot des laufenden Gastbetriebssystems reicht nicht;
					die virtuelle Maschine muss ggf. aus und erneut gestartet werden.
				</para>
				<para>
					Auf dem Reiter <guimenu>Erweitert</guimenu> der VM kann das CPU Modell explizit konfiguriert werden.
				</para>
				<para>
					Über die &ucsUCRV; <envar>uvmm/vm/cpu/host-model</envar> kann das Anpassen der virtuellen Maschinen automatisiert werden.
					Die folgenden Werte sind erlaubt:
				</para>
				<variablelist>
					<varlistentry>
						<term><literal>missing</literal></term>
						<listitem>
							<simpara>
								UVMM aktiviert die Überprüfung für alle virtuellen Maschinen, für die nicht bereits das CPU-Model explizit konfiguriert ist.
							</simpara>
						</listitem>
					</varlistentry>
					<varlistentry>
						<term><literal>always</literal></term>
						<listitem>
							<simpara>
								UVMM aktiviert die Überprüfung für alle virtuellen Maschinen, unabhängig davon, ob bereits ein CPU-Model explizit konfiguriert ist oder nicht.
								Die Überschreibt jede andere vorhandene Konfiguration eines CPU-Models.
							</simpara>
						</listitem>
					</varlistentry>
					<varlistentry>
						<term><literal>remove</literal></term>
						<listitem>
							<simpara>
								UVMM entfernt jede Konfiguration eines CPU-Models.
							</simpara>
						</listitem>
					</varlistentry>
					<varlistentry>
						<term>- nicht gesetzt -</term>
						<listitem>
							<simpara>
								UVMM konfiguriert kein virtuelle Maschinen um.
								Dies ist der Standard.
							</simpara>
						</listitem>
					</varlistentry>
				</variablelist>
				<caution>
					<para>
						Falls mehrere UVMM Dienste benutzt werden, so sollte die &ucsUCRV; <envar>uvmm/vm/cpu/host-model</envar> auf allen UCS-Systemen identisch gesetzt werden.
					</para>
				</caution>
			</section>
		</section>
	</section>

	<section id="uvmm::profile">
		<title>Profile</title>
		<para>
			Profile werden benutzt, um die anfänglichen Einstellungen für neue virtuelle Maschinen festzulegen.
			Unter anderem beinhalten diese folgende Einstellungen:
		</para>
		<itemizedlist>
			<listitem><simpara>Präfix für den Namen neuer virtueller Maschinen</simpara></listitem>
			<listitem><simpara>Anzahl der virtuellen <acronym>CPU</acronym>s</simpara></listitem>
			<listitem><simpara><acronym>CPU</acronym> Modell</simpara></listitem>
			<listitem><simpara>Standard-RAM-Größe</simpara></listitem>
			<listitem><simpara>Standardgröße für neue Festplatten-Images</simpara></listitem>
			<listitem><simpara>Standard-Boot-Reihenfolge für vollvirtualisierte virtuelle Maschinen</simpara></listitem>
			<listitem><simpara>Benutzung der paravirtualisierten Gerätetreiber</simpara></listitem>
			<listitem><simpara>Standardeinstellung für den Direktzugriff per <acronym>VNC</acronym></simpara></listitem>
			<listitem><simpara>Name der Netzwerk-Bridge-Schnittstelle</simpara></listitem>
		</itemizedlist>
		<para>
			Die UVMM-Profile werden aus dem <acronym>LDAP</acronym>-Verzeichnis gelesen und können dort auch angepasst werden.
			Zu finden sind die Profile im UMC-Modul <guimenu>LDAP-Verzeichnis</guimenu> im Container <uri>cn=Profiles,cn=Virtual Machine Manager</uri>.
			Dort können auch weitere Profile hinzugefügt werden.
		</para>
		<section id="uvmm::profile::network">
			<title>Ändern des Standardnetzwerkes</title>
			<para>
				Der Name der Bridge für das Standardnetzwerk ist in den <acronym>UVMM</acronym>-Profilen gespeichert.
				Wenn das Standardnetzwerk <filename class="devicefile">br0</filename> geändert wird, müssen diese angepasst werden.
				Der folgende Befehl aktualisiert alle Profile auf die Bridge <filename class="devicefile"><replaceable>$NEU</replaceable></filename>, die momentan als Netzwerkschnittstelle <filename class="devicefile"><replaceable>$ALT</replaceable></filename> benutzen:
			</para>
			<programlisting language="sh">
udm uvmm/profile list --filter interface="$ALT" |
	sed -ne 's/^DN: //p' |
	xargs -r -d '\n' -n 1 udm uvmm/profile modify --set interface="$NEU" --dn
			</programlisting>
		</section>
	</section>
</chapter>
